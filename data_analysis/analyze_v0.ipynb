{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./reviews_0-250.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset with a different encoding\n",
    "df = pd.read_csv('reviews_250-500.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Define good, bad, and neutral comments\n",
    "def classify_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return 'good'\n",
    "    elif rating <= 2:\n",
    "        return 'bad'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['sentiment'] = df['rating'].apply(classify_sentiment)\n",
    "\n",
    "# Group by 'product_name' and 'sentiment' and find the minimum size for each product\n",
    "grouped = df.groupby(['product_name', 'sentiment']).size().unstack(fill_value=0)\n",
    "grouped['min_size'] = grouped.min(axis=1)\n",
    "\n",
    "# Sample each group to the size of the smallest group for each product\n",
    "sampled_df_list = []\n",
    "for product, min_size in grouped['min_size'].items():\n",
    "    good_comments = df[(df['product_name'] == product) & (df['sentiment'] == 'good')].sample(min_size)\n",
    "    bad_comments = df[(df['product_name'] == product) & (df['sentiment'] == 'bad')].sample(min_size)\n",
    "    neutral_comments = df[(df['product_name'] == product) & (df['sentiment'] == 'neutral')].sample(min_size)\n",
    "    sampled_df_list.append(pd.concat([good_comments, bad_comments, neutral_comments]))\n",
    "\n",
    "# Combine all the sampled data\n",
    "sampled_df = pd.concat(sampled_df_list).reset_index(drop=True)\n",
    "\n",
    "# Save the reduced dataset to a new CSV file\n",
    "output_path = 'reduced_dataset_250-500.csv'\n",
    "sampled_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_158333/4186624788.py:5: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/media/yatipa_drive/sep_ana/reviews_1250-end.csv', encoding='ISO-8859-1')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment score for sephora beauty reviews\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('reduced_dataset.csv')\n",
    "\n",
    "# Map sentiment to numerical scores\n",
    "sentiment_mapping = {'good': 1, 'neutral': 0, 'bad': -1}\n",
    "data['sentiment_score'] = data['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Plotting the sentiment scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Count the occurrences of each sentiment score\n",
    "sentiment_counts = data['sentiment_score'].value_counts().sort_index()\n",
    "\n",
    "# Bar plot\n",
    "sentiment_counts.plot(kind='bar', color=['red', 'blue', 'green'])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Sentiment Score for Sephora Beauty Reviews')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Number of Reviews')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high frequent words in all reviews\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('reduced_dataset.csv')\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Combine all review texts into one string\n",
    "all_reviews = ' '.join(data['review_text'].dropna().astype(str))\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_tokenize(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to ASCII\n",
    "    text = unidecode(text)\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# Get the tokens\n",
    "tokens = clean_tokenize(all_reviews)\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(tokens)\n",
    "\n",
    "# Get the most common words\n",
    "most_common_words = word_counts.most_common(20)  # Adjust number as needed\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "word_freq_df = pd.DataFrame(most_common_words, columns=['word', 'frequency'])\n",
    "\n",
    "# Plotting the high-frequency words\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Bar plot\n",
    "plt.barh(word_freq_df['word'], word_freq_df['frequency'], color='skyblue')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('High-Frequency Words in Sephora Beauty Reviews')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "\n",
    "# Show the plot\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest frequency at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most common words in reviews by top 10 brand\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('reduced_dataset.csv')\n",
    "\n",
    "# Count the number of reviews per brand\n",
    "top_brands = data['brand_name'].value_counts().head(10)\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([char for char in text if char in string.printable])  # Remove non-ASCII characters\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# Prepare a dictionary to store word frequencies for each brand\n",
    "brand_word_counts = {}\n",
    "\n",
    "for brand in top_brands.index:\n",
    "    brand_reviews = ' '.join(data[data['brand_name'] == brand]['review_text'].dropna().astype(str))\n",
    "    tokens = clean_tokenize(brand_reviews)\n",
    "    word_counts = Counter(tokens)\n",
    "    brand_word_counts[brand] = word_counts.most_common(10)\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easier plotting\n",
    "word_freq_df = pd.DataFrame({\n",
    "    brand: dict(word_counts) for brand, word_counts in brand_word_counts.items()\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "# Plotting the high-frequency words for each brand\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, brand in enumerate(word_freq_df.columns):\n",
    "    ax = axes[i]\n",
    "    word_freq_df[brand].sort_values().plot(kind='barh', ax=ax, color='skyblue')\n",
    "    ax.set_title(f'Top 10 Words in Reviews for {brand}')\n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.set_ylabel('Words')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(word_freq_df.columns), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud in each brand (top 10)\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('reduced_dataset.csv')\n",
    "\n",
    "# Count the number of reviews per brand\n",
    "top_brands = data['brand_name'].value_counts().head(10)\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove non-ASCII characters\n",
    "    text = ''.join([char for char in text if char in string.printable])\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Join tokens back to string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Generate word clouds for the top 10 brands\n",
    "plt.figure(figsize=(20, 25))\n",
    "\n",
    "for i, brand in enumerate(top_brands.index, 1):\n",
    "    # Combine all reviews for the brand\n",
    "    brand_reviews = ' '.join(data[data['brand_name'] == brand]['review_text'].dropna().astype(str))\n",
    "    clean_reviews = clean_text(brand_reviews)\n",
    "    \n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(clean_reviews)\n",
    "    \n",
    "    # Plot the word cloud\n",
    "    plt.subplot(5, 2, i)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Word Cloud for {brand}', fontsize=16)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price impact on positive/negative comments\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('reduced_dataset.csv')\n",
    "\n",
    "# Categorize reviews as positive or negative based on the sentiment column\n",
    "data['sentiment_category'] = data['sentiment'].apply(lambda x: 'positive' if x == 'good' else 'negative')\n",
    "\n",
    "# Calculate the average price of products associated with positive and negative reviews\n",
    "average_price = data.groupby('sentiment_category')['price_usd'].mean()\n",
    "\n",
    "# Plotting the average prices for positive and negative reviews\n",
    "average_price.plot(kind='bar', figsize=(10, 6), color=['green', 'red'])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Price of Products for Positive and Negative Reviews')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Average Price (USD)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative/positive reviews in each brand (top 10)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('reduced_dataset.csv')\n",
    "\n",
    "# Count the number of reviews per brand\n",
    "top_brands = data['brand_name'].value_counts().head(10)\n",
    "\n",
    "# Categorize reviews as positive or negative based on the sentiment column\n",
    "data['sentiment_category'] = data['sentiment'].apply(lambda x: 'positive' if x == 'good' else 'negative')\n",
    "\n",
    "# Count the number of positive and negative reviews for each of the top 10 brands\n",
    "brand_sentiment_counts = data[data['brand_name'].isin(top_brands.index)].groupby(['brand_name', 'sentiment_category']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotting the positive and negative reviews for each brand\n",
    "brand_sentiment_counts.plot(kind='bar', stacked=True, figsize=(14, 8), color=['red', 'green'])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Number of Positive and Negative Reviews for Top 10 Brands')\n",
    "plt.xlabel('Brand Name')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.legend(title='Sentiment')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price impact on positive/negative comments\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('reduced_dataset.csv')\n",
    "\n",
    "# Categorize reviews as positive or negative based on the sentiment column\n",
    "data['sentiment_category'] = data['sentiment'].apply(lambda x: 'positive' if x == 'good' else 'negative')\n",
    "\n",
    "# Calculate the average price of products associated with positive and negative reviews\n",
    "average_price = data.groupby('sentiment_category')['price_usd'].mean()\n",
    "\n",
    "# Plotting the average prices for positive and negative reviews\n",
    "average_price.plot(kind='bar', figsize=(10, 6), color=['green', 'red'])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Price of Products for Positive and Negative Reviews')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Average Price (USD)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time series analysis(analyze the sentiment trend over time for reviews)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset with a different encoding\n",
    "df = pd.read_csv('reviews_0-250_new.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Define good, bad, and neutral comments\n",
    "def classify_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return 'good'\n",
    "    elif rating <= 2:\n",
    "        return 'bad'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['sentiment'] = df['rating'].apply(classify_sentiment)\n",
    "\n",
    "# Group by 'product_name' and 'sentiment' and find the minimum size for each product\n",
    "grouped = df.groupby(['product_name', 'sentiment']).size().unstack(fill_value=0)\n",
    "grouped['min_size'] = grouped.min(axis=1)\n",
    "\n",
    "# Sample each group to the size of the smallest group for each product\n",
    "sampled_df_list = []\n",
    "for product, min_size in grouped['min_size'].iteritems():\n",
    "    good_comments = df[(df['product_name'] == product) & (df['sentiment'] == 'good')].sample(min_size)\n",
    "    bad_comments = df[(df['product_name'] == product) & (df['sentiment'] == 'bad')].sample(min_size)\n",
    "    neutral_comments = df[(df['product_name'] == product) & (df['sentiment'] == 'neutral')].sample(min_size)\n",
    "    sampled_df_list.append(pd.concat([good_comments, bad_comments, neutral_comments]))\n",
    "\n",
    "# Combine all the sampled data\n",
    "sampled_df = pd.concat(sampled_df_list).reset_index(drop=True)\n",
    "\n",
    "# Save the reduced dataset to a new CSV file\n",
    "output_path = 'reduced_dataset(1).csv'\n",
    "sampled_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('reduced_dataset(1).csv')\n",
    "\n",
    "# Inspect column names\n",
    "print(data.columns)\n",
    "\n",
    "# the date column name is 'submission_time'\n",
    "data['submission_time'] = pd.to_datetime(data['submission_time'])\n",
    "\n",
    "# Categorize reviews as positive or negative based on the sentiment column\n",
    "data['sentiment_category'] = data['sentiment'].apply(lambda x: 'positive' if x == 'good' else 'negative')\n",
    "\n",
    "# Group by date and sentiment category, then count the number of reviews per group\n",
    "sentiment_trend = data.groupby([data['submission_time'].dt.to_period('M'), 'sentiment_category']).size().unstack(fill_value=0)\n",
    "\n",
    "# Convert the period index back to a datetime index for plotting\n",
    "sentiment_trend.index = sentiment_trend.index.to_timestamp()\n",
    "\n",
    "# Plotting the time series of sentiment trends\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot positive sentiment trend\n",
    "plt.plot(sentiment_trend.index, sentiment_trend['positive'], label='Positive', color='green')\n",
    "\n",
    "# Plot negative sentiment trend\n",
    "plt.plot(sentiment_trend.index, sentiment_trend['negative'], label='Negative', color='red')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Sentiment Trend Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.legend(title='Sentiment')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Distribution by Category (Show the distribution of sentiment scores across different product categories.) Top 10 product\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('reduced_dataset.csv')\n",
    "\n",
    "# 'sentiment' column contains categorical sentiment labels ('good', 'neutral', 'bad')\n",
    "# Convert sentiment labels to numeric scores for plotting purposes\n",
    "sentiment_scores = {'good': 3, 'neutral': 2, 'bad': 1}  # Define mapping of sentiment labels to scores\n",
    "data['sentiment_score'] = data['sentiment'].map(sentiment_scores)\n",
    "\n",
    "# Identify top 10 products with the most reviews\n",
    "top_products = data['product_name'].value_counts().nlargest(10).index\n",
    "\n",
    "# Filter data to include only top products\n",
    "filtered_data = data[data['product_name'].isin(top_products)]\n",
    "\n",
    "# Plotting a box plot to show sentiment distribution by top products\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Customize colors for each category\n",
    "colors = sns.color_palette(\"Set3\", len(top_products))\n",
    "\n",
    "# Create the box plot\n",
    "sns.boxplot(data=filtered_data, x='product_name', y='sentiment_score', palette=colors)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Sentiment Distribution by Top 10 Products')\n",
    "plt.xlabel('Product Name')\n",
    "plt.ylabel('Sentiment Score')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Load the reduced dataset\n",
    "df = pd.read_csv('reduced_dataset.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df['review_text'].fillna('', inplace=True)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove non-English characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['cleaned_review_text'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "# Function to detect language and filter out non-English reviews\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "# Filter out non-English reviews\n",
    "df = df[df['cleaned_review_text'].apply(is_english)]\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['sentiment'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# Tokenize the cleaned text data\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['cleaned_review_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_review_text'])\n",
    "\n",
    "# Pad the sequences\n",
    "max_length = 100\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['sentiment'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(filepath, word_index, embedding_dim=300):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Use the 300-dimensional GloVe embeddings\n",
    "glove_filepath = 'glove.42B.300d.txt'\n",
    "embedding_dim = 300\n",
    "embedding_matrix = load_glove_embeddings(glove_filepath, tokenizer.word_index, embedding_dim)\n",
    "\n",
    "# Build the model with GloVe embeddings\n",
    "def build_model_with_glove():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.7),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.7),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dropout(0.7),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model_with_glove()\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 25\n",
    "batch_size = 256\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Learning Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_probs[:, i], pos_label=i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['blue', 'green', 'red']\n",
    "for i, color in enumerate(colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, label=f'ROC curve for class {i} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
